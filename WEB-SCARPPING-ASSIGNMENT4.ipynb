{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1. Scrape the details of most viewed videos on YouTube from Wikipedia:\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos/\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1.', '\"Baby Shark Dance\"[22]', \"Pinkfong Kids' Songs & Stories\", '8.09', 'June 17, 2016', '[B]']\n",
      "['2.', '\"Despacito\"[24]', 'Luis Fonsi', '7.24', 'January 12, 2017', '[C]']\n",
      "['3.', '\"Shape of You\"[25]', 'Ed Sheeran', '5.21', 'January 30, 2017', '[D]']\n",
      "['4.', '\"See You Again\"[26]', 'Wiz Khalifa', '4.99', 'April 6, 2015', '[E]']\n",
      "['5.', '\"Johny Johny Yes Papa\"[29]', 'LooLoo Kids', '4.90', 'October 8, 2016', '[F]']\n",
      "['6.', '\"Masha and the Bear – Recipe for Disaster\"[30]', 'Get Movies', '4.42', 'January 31, 2012', '[G]']\n",
      "['7.', '\"Uptown Funk\"[31]', 'Mark Ronson', '4.11', 'November 19, 2014', '[H]']\n",
      "['8.', '\"Gangnam Style\"[32]', 'Psy', '4.00', 'July 15, 2012', '[I]']\n",
      "['9.', '\"Learning Colors – Colorful Eggs on a Farm\"[34]', 'Miroshka TV', '3.74', 'February 27, 2018', '']\n",
      "['10.', '\"Bath Song\"[35]', 'Cocomelon – Nursery Rhymes', '3.74', 'May 2, 2018', '']\n",
      "['11.', '\"Phonics Song with Two Words\"[36]', 'ChuChu TV', '3.60', 'March 6, 2014', '']\n",
      "['12.', '\"Sorry\"[37]', 'Justin Bieber', '3.41', 'October 22, 2015', '']\n",
      "['13.', '\"Sugar\"[38]', 'Maroon 5', '3.41', 'January 14, 2015', '']\n",
      "['14.', '\"Roar\"[39]', 'Katy Perry', '3.29', 'September 5, 2013', '']\n",
      "['15.', '\"Counting Stars\"[40]', 'OneRepublic', '3.21', 'May 31, 2013', '']\n",
      "['16.', '\"Thinking Out Loud\"[41]', 'Ed Sheeran', '3.20', 'October 7, 2014', '']\n",
      "['17.', '\"Dame Tu Cosita\"[42]', 'El Chombo', '3.15', 'April 5, 2018', '']\n",
      "['18.', '\"Shake It Off\"[43]', 'Taylor Swift', '3.03', 'August 18, 2014', '']\n",
      "['19.', '\"Faded\"[44]', 'Alan Walker', '3.00', 'December 3, 2015', '']\n",
      "['20.', '\"Dark Horse\"[45]', 'Katy Perry', '2.99', 'February 20, 2014', '']\n",
      "['21.', '\"Lean On\"[46]', 'Major Lazer Official', '2.99', 'March 22, 2015', '']\n",
      "['22.', '\"Bailando\"[47]', 'Enrique Iglesias', '2.98', 'April 11, 2014', '']\n",
      "['23.', '\"Girls Like You\"[48]', 'Maroon 5', '2.97', 'May 31, 2018', '']\n",
      "['24.', '\"Let Her Go\"[49]', 'Passenger', '2.93', 'July 25, 2012', '']\n",
      "['25.', '\"Mi Gente\"[50]', 'J Balvin', '2.88', 'June 29, 2017', '']\n",
      "['26.', '\"Hello\"[51]', 'Adele', '2.80', 'October 22, 2015', '']\n",
      "['27.', '\"Perfect\"[52]', 'Ed Sheeran', '2.76', 'November 9, 2017', '']\n",
      "['28.', '\"Waka Waka (This Time for Africa)\"[53]', 'Shakira', '2.76', 'June 4, 2010', '']\n",
      "['29.', '\"Blank Space\"[54]', 'Taylor Swift', '2.73', 'November 10, 2014', '']\n",
      "['30.', '\"Chantaje\"[55]', 'Shakira', '2.63', 'November 18, 2016', '']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "table=soup.find('table', {'class':'wikitable sortable'}).tbody\n",
    "\n",
    "rows=table.find_all('tr')\n",
    "\n",
    "columns=[v.text.replace('\\n','') for v in rows[0].find_all('th') ]\n",
    "\n",
    "df=pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(1,len(rows)):\n",
    "    tds=rows[i].find_all('td')\n",
    "    values=[td.text.replace('\\n','') for td in tds]\n",
    "    print(values)\n",
    "    \n",
    "##df=df.append(pd.Series(values, index=columns),ignore_index=True)\n",
    "##    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2. Scrape the details team India’s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1\n",
    "st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://www.bcci.tv/\")\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/div[3]/div/div[2]/div[2]/nav/ul/li[1]/div[2]\").click()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3. Scrape the details of selenium exception from guru99.com.\n",
    "Url = https://www.guru99.com/\n",
    "You need to find following details:\n",
    "A) Name\n",
    "B) Description\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://www.guru99.com/\")\n",
    "\n",
    "link=driver.find_element_by_link_text(\"Selenium\")\n",
    "link.click()\n",
    "\n",
    "link1=driver.find_element_by_xpath('//*[@id=\"g-mainbar\"]/div/div/div/div/div/div/div[2]/table[5]/tbody/tr[34]/td[1]/a').click()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ElementNotVisibleException ', 'This type of Selenium exception occurs when an existing element in DOM has a feature set as hidden. ']\n",
      "['ElementNotSelectableException ', 'This Selenium exception occurs when an element is presented in the DOM, but you can be able to select. Therefore, it is not possible to interact. ']\n",
      "['NoSuchElementException ', 'This Exception occurs if an element could not be found. ']\n",
      "['NoSuchFrameException ', 'This Exception occurs if the frame target to be switched to does not exist. ']\n",
      "['NoAlertPresentException ', 'This Exception occurs when you switch to no presented alert. ']\n",
      "['NoSuchWindowException ', 'This Exception occurs if the window target to be switch does not exist. ']\n",
      "['StaleElementReferenceException ', 'This Selenium exception occurs happens when the web element is detached from the current DOM. ']\n",
      "['SessionNotFoundException ', 'The WebDriver is acting after you quit the browser. ']\n",
      "['TimeoutException ', \"Thrown when there is not enough time for a command to be completed. For Example, the element searched wasn't found in the specified time. \"]\n",
      "['WebDriverException ', 'This Exception takes place when the WebDriver is acting right after you close the browser. ']\n",
      "['ConnectionClosedException ', 'This type of Exception takes place when there is a disconnection in the driver. ']\n",
      "['ElementClickInterceptedException ', 'The command may not be completed as the element receiving the events is concealing the element which was requested clicked. ']\n",
      "['ElementNotInteractableException ', 'This Selenium exception is thrown when any element is presented in the DOM. However, it is impossible to interact with such an element. ']\n",
      "['ErrorInResponseException ', 'This happens while interacting with the Firefox extension or the remote driver server. ']\n",
      "['ErrorHandler.UnknownServerException ', 'Exception is used as a placeholder in case if the server returns an error without a stack trace. ']\n",
      "['ImeActivationFailedException ', 'This expectation will occur when IME engine activation has failed. ']\n",
      "['ImeNotAvailableException ', 'It takes place when IME support is unavailable. ']\n",
      "['InsecureCertificateException ', 'Navigation made the user agent to hit a certificate warning. This can cause by an invalid or expired TLS certificate. ']\n",
      "['InvalidArgumentException ', 'It occurs when an argument does not belong to the expected type. ']\n",
      "['InvalidCookieDomainException ', 'This happens when you try to add a cookie under a different domain instead of current URL. ']\n",
      "['InvalidCoordinatesException ', 'This type of Exception matches an interacting operation that is not valid. ']\n",
      "['InvalidElementStateExceptio ', \"It occurs when command can't be finished when the element is invalid. \"]\n",
      "['InvalidSessionIdException ', 'This Exception took place when the given session ID is not included in the list of active sessions. It means the session does not exist or is inactive either. ']\n",
      "['InvalidSwitchToTargetException ', 'This occurs when the frame or window target to be switched does not exist. ']\n",
      "['JavascriptException ', 'This issue occurs while executing JavaScript given by the user. ']\n",
      "['JsonException ', 'It occurs when you afford to get the session when the session is not created. ']\n",
      "['NoSuchAttributeException ', 'This kind of Exception occurs when the attribute of an element could not be found. ']\n",
      "['MoveTargetOutOfBoundsException ', 'It takes place if the target provided to the ActionChains move() methodology is not valid. For Example, out of the document. ']\n",
      "['NoSuchContextException ', 'ContextAware does mobile device testing. ']\n",
      "['NoSuchCookieException ', 'This Exception occurs when no cookie matching with the given pathname found for all the associated cookies of the currently browsing document. ']\n",
      "['NotFoundException ', 'This Exception is a subclass of WebDriverException. This will occur when an element on the DOM does not exist. ']\n",
      "['RemoteDriverServerException ', 'This Selenium exception is thrown when the server is not responding because of the problem that the capabilities described are not proper. ']\n",
      "['ScreenshotException ', 'It is not possible to capture a screen. ']\n",
      "['SessionNotCreatedException ', 'It happens when a new session could not be successfully created. ']\n",
      "['UnableToSetCookieException ', 'This occurs if a driver is unable to set a cookie. ']\n",
      "['UnexpectedTagNameException ', 'Happens if a support class did not get a web element as expected. ']\n",
      "['UnhandledAlertException ', 'This expectation occurs when there is an alert, but WebDriver is not able to perform Alert operation. ']\n",
      "['UnexpectedAlertPresentException ', 'It occurs when there is the appearance of an unexpected alert. ']\n",
      "['UnknownMethodException ', 'This Exception happens when the requested command matches with a known URL but and not matching with a methodology for a specific URL. ']\n",
      "['UnreachableBrowserException ', 'This Exception occurs only when the browser is not able to be opened or crashed because of some reason. ']\n",
      "['UnsupportedCommandException ', \"This occurs when remote WebDriver does n't send valid commands as expected. \"]\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.guru99.com/exception-handling-selenium.html\"\n",
    "\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "table=soup.find('table', {'class':'table table-striped'})\n",
    "\n",
    "rows=table.find_all('tr')\n",
    "\n",
    "columns=[v.text.replace('\\n','') for v in rows[0].find_all('td') ]\n",
    "df=pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(1,len(rows)):\n",
    "    tds=rows[i].find_all('td')\n",
    "    values=[td.text.replace('\\n','') for td in tds]\n",
    "    print(values)\n",
    "    ##df=df.append(pd.Series(values, index=columns),ignore_index=True)\n",
    "    ##print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "Url = http://statisticstimes.com/\n",
    "You have to find following details:\n",
    "A) Rank\n",
    "B) State\n",
    "C) GSDP(18-19)\n",
    "D) GSDP(17-18)\n",
    "E) Share(2017)\n",
    "F) GDP($ billion)\n",
    "Note: - From statisticstimes home page you have to reach to economy page through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "##code for reaching the website\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"http://statisticstimes.com/\")\n",
    "\n",
    "selectV=driver.find_element_by_xpath('//*[@id=\"top\"]/div[2]/div[2]/button').click()\n",
    "\n",
    "link=driver.find_element_by_link_text(\"India\")\n",
    "link.click()\n",
    "\n",
    "link1=driver.find_element_by_xpath('/html/body/div[2]/div[2]/div[2]/ul/li[1]/a').click()\n",
    "print(link1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GSDP\n",
      "\n",
      "Rank\n",
      "State\n",
      "GSDP (Cr INR at Current prices)\n",
      "ShareGDP ($billion)\n",
      "GSDP (Cr INR at 2011-12 prices)\n",
      "19-2018-19\n",
      "18-19201919-2018-19\n",
      "\n",
      "\n",
      "1Maharashtra\n",
      "-\n",
      "2,632,792\n",
      "13.88%\n",
      "398.145\n",
      "-\n",
      "2,039,074\n",
      "2Tamil Nadu\n",
      "1,845,853\n",
      "1,630,208\n",
      "8.59%\n",
      "246.529\n",
      "1,312,929\n",
      "1,215,307\n",
      "3Uttar Pradesh\n",
      "1,687,818\n",
      "1,584,764\n",
      "8.35%\n",
      "239.656\n",
      "1,166,817\n",
      "1,123,982\n",
      "4Gujarat\n",
      "-\n",
      "1,502,899\n",
      "7.92%\n",
      "227.276\n",
      "-\n",
      "1,186,379\n",
      "5Karnataka\n",
      "1,631,977\n",
      "1,493,127\n",
      "7.87%\n",
      "225.798\n",
      "1,156,039\n",
      "1,091,077\n",
      "6West Bengal\n",
      "1,253,832\n",
      "1,089,898\n",
      "5.75%\n",
      "164.820\n",
      "793,223\n",
      "739,525\n",
      "7Rajasthan\n",
      "1,020,989\n",
      "942,586\n",
      "4.97%\n",
      "142.543\n",
      "711,627\n",
      "677,428\n",
      "8Andhra Pradesh\n",
      "972,782\n",
      "862,957\n",
      "4.55%\n",
      "130.501\n",
      "672,018\n",
      "621,301\n",
      "9Telangana\n",
      "969,604\n",
      "861,031\n",
      "4.54%\n",
      "130.210\n",
      "663,258\n",
      "612,828\n",
      "10Madhya Pradesh\n",
      "906,672\n",
      "809,592\n",
      "4.27%\n",
      "122.431\n",
      "561,801\n",
      "522,009\n",
      "11Kerala\n",
      "-\n",
      "781,653\n",
      "4.12%\n",
      "118.206\n",
      "-\n",
      "559,412\n",
      "12Delhi\n",
      "856,112\n",
      "774,870\n",
      "4.08%\n",
      "117.180\n",
      "634,408\n",
      "590,569\n",
      "13Haryana\n",
      "831,610\n",
      "734,163\n",
      "3.87%\n",
      "111.024\n",
      "572,240\n",
      "531,085\n",
      "14Bihar\n",
      "611,804\n",
      "530,363\n",
      "2.80%\n",
      "80.204\n",
      "414,977\n",
      "375,651\n",
      "15Punjab\n",
      "574,760\n",
      "526,376\n",
      "2.77%\n",
      "79.601\n",
      "418,868\n",
      "397,669\n",
      "16Odisha\n",
      "521,275\n",
      "487,805\n",
      "2.57%\n",
      "73.768\n",
      "396,499\n",
      "376,877\n",
      "17Assam\n",
      "-\n",
      "315,881\n",
      "1.67%\n",
      "47.769\n",
      "-\n",
      "234,048\n",
      "18Chhattisgarh\n",
      "329,180\n",
      "304,063\n",
      "1.60%\n",
      "45.982\n",
      "243,477\n",
      "231,182\n",
      "19Jharkhand\n",
      "328,598\n",
      "297,204\n",
      "1.57%\n",
      "44.945\n",
      "240,036\n",
      "224,986\n",
      "20Uttarakhand\n",
      "-\n",
      "245,895\n",
      "1.30%\n",
      "37.186\n",
      "-\n",
      "193,273\n",
      "21Jammu & Kashmir\n",
      "-\n",
      "155,956\n",
      "0.82%\n",
      "23.584\n",
      "-\n",
      "112,755\n",
      "22Himachal Pradesh\n",
      "165,472\n",
      "153,845\n",
      "0.81%\n",
      "23.265\n",
      "124,403\n",
      "117,851\n",
      "23Goa\n",
      "80,449\n",
      "73,170\n",
      "0.39%\n",
      "11.065\n",
      "63,408\n",
      "57,787\n",
      "24Tripura\n",
      "55,984\n",
      "49,845\n",
      "0.26%\n",
      "7.538\n",
      "40,583\n",
      "36,963\n",
      "25Chandigarh\n",
      "-\n",
      "42,114\n",
      "0.22%\n",
      "6.369\n",
      "-\n",
      "31,192\n",
      "26Puducherry\n",
      "38,253\n",
      "34,433\n",
      "0.18%\n",
      "5.207\n",
      "25,093\n",
      "23,013\n",
      "27Meghalaya\n",
      "36,572\n",
      "33,481\n",
      "0.18%\n",
      "5.063\n",
      "26,695\n",
      "24,682\n",
      "28Sikkim\n",
      "32,496\n",
      "28,723\n",
      "0.15%\n",
      "4.344\n",
      "20,017\n",
      "18,722\n",
      "29Manipur\n",
      "31,790\n",
      "27,870\n",
      "0.15%\n",
      "4.215\n",
      "20,673\n",
      "19,300\n",
      "30Nagaland\n",
      "-\n",
      "27,283\n",
      "0.14%\n",
      "4.126\n",
      "-\n",
      "17,647\n",
      "31Arunachal Pradesh\n",
      "-\n",
      "24,603\n",
      "0.13%\n",
      "3.721\n",
      "-\n",
      "16,676\n",
      "32Mizoram\n",
      "26,503\n",
      "22,287\n",
      "0.12%\n",
      "3.370\n",
      "18,797\n",
      "16,478\n",
      "33Andaman & Nicobar Islands\n",
      "---\n",
      "---\n",
      "\n",
      "\n",
      "India\n",
      "20,339,849\n",
      "18,971,237\n",
      "2,869\n",
      "14,565,951\n",
      "13,981,426\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"http://statisticstimes.com/economy/india/indian-states-gdp.php\"\n",
    "\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "table=soup.find('table', {'id':'table_id'})\n",
    "print(table.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5. Scrape the details of trending repositories on Github.com.\n",
    "Url = https://github.com/\n",
    "You have to find the following details:\n",
    "A) Repository title\n",
    "B) Repository description\n",
    "C) Contributors count\n",
    "D) Language used\n",
    "ASSIGNMENT\n",
    "Note: - From the home page you have to click on the trending option from Explore menu through code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6. Scrape the details of top 100 songs on billiboard.com.\n",
    "Url = https://www.billboard.com/\n",
    "You have to find the following details:\n",
    "A) Song name\n",
    "B) Artist name\n",
    "C) Last week rank\n",
    "D) Peak rank\n",
    "E) Weeks on board\n",
    "Note: - From the home page you have to click on the charts option then hot 100-page link through code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##code for reaching website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Song Name</th>\n",
       "      <th>Artist Name</th>\n",
       "      <th>Last Week Rank</th>\n",
       "      <th>Peak Rank</th>\n",
       "      <th>Weeks on Board</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Drivers License</td>\n",
       "      <td>Olivia Rodrigo</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Up</td>\n",
       "      <td>Cardi B</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Go Crazy</td>\n",
       "      <td>Chris Brown &amp; Young Thug</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34+35</td>\n",
       "      <td>Ariana Grande</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Blinding Lights</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Save Your Tears</td>\n",
       "      <td>The Weeknd</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mood</td>\n",
       "      <td>24kGoldn Featuring iann dior</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Calling My Phone</td>\n",
       "      <td>Lil Tjay Featuring 6LACK</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Positions</td>\n",
       "      <td>Ariana Grande</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Levitating</td>\n",
       "      <td>Dua Lipa Featuring DaBaby</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Song Name                   Artist Name Last Week Rank Peak Rank  \\\n",
       "0   Drivers License                Olivia Rodrigo              1         1   \n",
       "1                Up                       Cardi B              5         2   \n",
       "2          Go Crazy      Chris Brown & Young Thug              8         3   \n",
       "3             34+35                 Ariana Grande              2         2   \n",
       "4   Blinding Lights                    The Weeknd              4         1   \n",
       "5   Save Your Tears                    The Weeknd              6         4   \n",
       "6              Mood  24kGoldn Featuring iann dior              7         1   \n",
       "7  Calling My Phone      Lil Tjay Featuring 6LACK              3         3   \n",
       "8         Positions                 Ariana Grande             10         1   \n",
       "9        Levitating     Dua Lipa Featuring DaBaby              9         5   \n",
       "\n",
       "  Weeks on Board  \n",
       "0              7  \n",
       "1              3  \n",
       "2             42  \n",
       "3             17  \n",
       "4             64  \n",
       "5             11  \n",
       "6             29  \n",
       "7              2  \n",
       "8             18  \n",
       "9             21  "
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.billboard.com/charts/hot-100\"\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "songs=soup.findAll('li', {'class':'chart-list__element display--flex'})\n",
    "SongName=[]\n",
    "ArtistName=[]\n",
    "LastWeekRank=[]\n",
    "PeakRank=[]\n",
    "WeeksonBard=[]\n",
    "\n",
    "for i in songs:\n",
    "    SongName.append(i.find('span',{'class':'chart-element__information__song text--truncate color--primary'}).text)\n",
    "    ArtistName.append(i.find('span',{'class':'chart-element__information__artist text--truncate color--secondary'}).text)\n",
    "    LastWeekRank.append(i.find('span',{'class':'chart-element__meta text--center color--secondary text--last'}).text)\n",
    "    PeakRank.append(i.find('span',{'class':'chart-element__meta text--center color--secondary text--peak'}).text)\n",
    "    WeeksonBard.append(i.find('span',{'class':'chart-element__meta text--center color--secondary text--week'}).text)\n",
    "   \n",
    "data=list(zip(SongName,ArtistName,LastWeekRank,PeakRank,WeeksonBard))\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(data,columns=['Song Name','Artist Name','Last Week Rank','Peak Rank','Weeks on Board'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7. Scrape the details of Data science recruiters from naukri.com.\n",
    "Url = https://www.naukri.com/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Designation\n",
    "C) Company\n",
    "D) Skills they hire for\n",
    "E) Location\n",
    "Note: - From naukri.com homepage click on the recruiters option and the on the search pane type Data science and\n",
    "click on search. All this should be done through code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8. Scrape the details of Highest selling novels.\n",
    "Url = https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\n",
    "You have to find the following details:\n",
    "A) Book name\n",
    "B) Author name\n",
    "C) Volumes sold\n",
    "D) Publisher\n",
    "E) Genre\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' 1 ', ' Da Vinci Code,The ', ' Brown, Dan ', ' 5,094,805 ', ' Transworld ', ' Crime, Thriller & Adventure ']\n",
      "[' 2 ', ' Harry Potter and the Deathly Hallows ', ' Rowling, J.K. ', ' 4,475,152 ', ' Bloomsbury ', \" Children's Fiction \"]\n",
      "[' 3 ', \" Harry Potter and the Philosopher's Stone \", ' Rowling, J.K. ', ' 4,200,654 ', ' Bloomsbury ', \" Children's Fiction \"]\n",
      "[' 4 ', ' Harry Potter and the Order of the Phoenix ', ' Rowling, J.K. ', ' 4,179,479 ', ' Bloomsbury ', \" Children's Fiction \"]\n",
      "[' 5 ', ' Fifty Shades of Grey ', ' James, E. L. ', ' 3,758,936 ', ' Random House ', ' Romance & Sagas ']\n",
      "[' 6 ', ' Harry Potter and the Goblet of Fire ', ' Rowling, J.K. ', ' 3,583,215 ', ' Bloomsbury ', \" Children's Fiction \"]\n",
      "[' 7 ', ' Harry Potter and the Chamber of Secrets ', ' Rowling, J.K. ', ' 3,484,047 ', ' Bloomsbury ', \" Children's Fiction \"]\n",
      "[' 8 ', ' Harry Potter and the Prisoner of Azkaban ', ' Rowling, J.K. ', ' 3,377,906 ', ' Bloomsbury ', \" Children's Fiction \"]\n",
      "[' 9 ', ' Angels and Demons ', ' Brown, Dan ', ' 3,193,946 ', ' Transworld ', ' Crime, Thriller & Adventure ']\n",
      "[' 10 ', \" Harry Potter and the Half-blood Prince:Children's Edition \", ' Rowling, J.K. ', ' 2,950,264 ', ' Bloomsbury ', \" Children's Fiction \"]\n",
      "[' 11 ', ' Fifty Shades Darker ', ' James, E. L. ', ' 2,479,784 ', ' Random House ', ' Romance & Sagas ']\n",
      "[' 12 ', ' Twilight ', ' Meyer, Stephenie ', ' 2,315,405 ', ' Little, Brown Book ', ' Young Adult Fiction ']\n",
      "[' 13 ', ' Girl with the Dragon Tattoo,The:Millennium Trilogy ', ' Larsson, Stieg ', ' 2,233,570 ', ' Quercus ', ' Crime, Thriller & Adventure ']\n",
      "[' 14 ', ' Fifty Shades Freed ', ' James, E. L. ', ' 2,193,928 ', ' Random House ', ' Romance & Sagas ']\n",
      "[' 15 ', ' Lost Symbol,The ', ' Brown, Dan ', ' 2,183,031 ', ' Transworld ', ' Crime, Thriller & Adventure ']\n",
      "[' 16 ', ' New Moon ', ' Meyer, Stephenie ', ' 2,152,737 ', ' Little, Brown Book ', ' Young Adult Fiction ']\n",
      "[' 17 ', ' Deception Point ', ' Brown, Dan ', ' 2,062,145 ', ' Transworld ', ' Crime, Thriller & Adventure ']\n",
      "[' 18 ', ' Eclipse ', ' Meyer, Stephenie ', ' 2,052,876 ', ' Little, Brown Book ', ' Young Adult Fiction ']\n",
      "[' 19 ', ' Lovely Bones,The ', ' Sebold, Alice ', ' 2,005,598 ', ' Pan Macmillan ', ' General & Literary Fiction ']\n",
      "[' 20 ', ' Curious Incident of the Dog in the Night-time,The ', ' Haddon, Mark ', ' 1,979,552 ', ' Random House ', ' General & Literary Fiction ']\n",
      "[' 21 ', ' Digital Fortress ', ' Brown, Dan ', ' 1,928,900 ', ' Transworld ', ' Crime, Thriller & Adventure ']\n",
      "[' 22 ', ' Short History of Nearly Everything,A ', ' Bryson, Bill ', ' 1,852,919 ', ' Transworld ', ' Popular Science ']\n",
      "[' 23 ', ' Girl Who Played with Fire,The:Millennium Trilogy ', ' Larsson, Stieg ', ' 1,814,784 ', ' Quercus ', ' Crime, Thriller & Adventure ']\n",
      "[' 24 ', ' Breaking Dawn ', ' Meyer, Stephenie ', ' 1,787,118 ', ' Little, Brown Book ', ' Young Adult Fiction ']\n",
      "[' 25 ', ' Very Hungry Caterpillar,The:The Very Hungry Caterpillar ', ' Carle, Eric ', ' 1,783,535 ', ' Penguin ', ' Picture Books ']\n",
      "[' 26 ', ' Gruffalo,The ', ' Donaldson, Julia ', ' 1,781,269 ', ' Pan Macmillan ', ' Picture Books ']\n",
      "[' 27 ', \" Jamie's 30-Minute Meals \", ' Oliver, Jamie ', ' 1,743,266 ', ' Penguin ', ' Food & Drink: General ']\n",
      "[' 28 ', ' Kite Runner,The ', ' Hosseini, Khaled ', ' 1,629,119 ', ' Bloomsbury ', ' General & Literary Fiction ']\n",
      "[' 29 ', ' One Day ', ' Nicholls, David ', ' 1,616,068 ', ' Hodder & Stoughton ', ' General & Literary Fiction ']\n",
      "[' 30 ', ' Thousand Splendid Suns,A ', ' Hosseini, Khaled ', ' 1,583,992 ', ' Bloomsbury ', ' General & Literary Fiction ']\n",
      "[' 31 ', \" Girl Who Kicked the Hornets' Nest,The:Millennium Trilogy \", ' Larsson, Stieg ', ' 1,555,135 ', ' Quercus ', ' Crime, Thriller & Adventure ']\n",
      "[' 32 ', \" Time Traveler's Wife,The \", ' Niffenegger, Audrey ', ' 1,546,886 ', ' Random House ', ' General & Literary Fiction ']\n",
      "[' 33 ', ' Atonement ', ' McEwan, Ian ', ' 1,539,428 ', ' Random House ', ' General & Literary Fiction ']\n",
      "[' 34 ', \" Bridget Jones's Diary:A Novel \", ' Fielding, Helen ', ' 1,508,205 ', ' Pan Macmillan ', ' General & Literary Fiction ']\n",
      "[' 35 ', ' World According to Clarkson,The ', ' Clarkson, Jeremy ', ' 1,489,403 ', ' Penguin ', ' Humour: Collections & General ']\n",
      "[' 36 ', \" Captain Corelli's Mandolin \", ' Bernieres, Louis de ', ' 1,352,318 ', ' Random House ', ' General & Literary Fiction ']\n",
      "[' 37 ', ' Sound of Laughter,The ', ' Kay, Peter ', ' 1,310,207 ', ' Random House ', ' Autobiography: General ']\n",
      "[' 38 ', ' Life of Pi ', ' Martel, Yann ', ' 1,310,176 ', ' Canongate ', ' General & Literary Fiction ']\n",
      "[' 39 ', ' Billy Connolly ', ' Stephenson, Pamela ', ' 1,231,957 ', ' HarperCollins ', ' Biography: The Arts ']\n",
      "[' 40 ', ' Child Called It,A ', ' Pelzer, Dave ', ' 1,217,712 ', ' Orion ', ' Autobiography: General ']\n",
      "[' 41 ', \" Gruffalo's Child,The \", ' Donaldson, Julia ', ' 1,208,711 ', ' Pan Macmillan ', ' Picture Books ']\n",
      "[' 42 ', \" Angela's Ashes:A Memoir of a Childhood \", ' McCourt, Frank ', ' 1,204,058 ', ' HarperCollins ', ' Autobiography: General ']\n",
      "[' 43 ', ' Birdsong ', ' Faulks, Sebastian ', ' 1,184,967 ', ' Random House ', ' General & Literary Fiction ']\n",
      "[' 44 ', ' Northern Lights:His Dark Materials S. ', ' Pullman, Philip ', ' 1,181,503 ', ' Scholastic Ltd. ', ' Young Adult Fiction ']\n",
      "[' 45 ', ' Labyrinth ', ' Mosse, Kate ', ' 1,181,093 ', ' Orion ', ' General & Literary Fiction ']\n",
      "[' 46 ', ' Harry Potter and the Half-blood Prince ', ' Rowling, J.K. ', ' 1,153,181 ', ' Bloomsbury ', ' Science Fiction & Fantasy ']\n",
      "[' 47 ', ' Help,The ', ' Stockett, Kathryn ', ' 1,132,336 ', ' Penguin ', ' General & Literary Fiction ']\n",
      "[' 48 ', ' Man and Boy ', ' Parsons, Tony ', ' 1,130,802 ', ' HarperCollins ', ' General & Literary Fiction ']\n",
      "[' 49 ', ' Memoirs of a Geisha ', ' Golden, Arthur ', ' 1,126,337 ', ' Random House ', ' General & Literary Fiction ']\n",
      "[' 50 ', \" No.1 Ladies' Detective Agency,The:No.1 Ladies' Detective Agency S. \", ' McCall Smith, Alexander ', ' 1,115,549 ', ' Little, Brown Book ', ' Crime, Thriller & Adventure ']\n",
      "[' 51 ', ' Island,The ', ' Hislop, Victoria ', ' 1,108,328 ', ' Headline ', ' General & Literary Fiction ']\n",
      "[' 52 ', ' PS, I Love You ', ' Ahern, Cecelia ', ' 1,107,379 ', ' HarperCollins ', ' General & Literary Fiction ']\n",
      "[' 53 ', ' You are What You Eat:The Plan That Will Change Your Life ', ' McKeith, Gillian ', ' 1,104,403 ', ' Penguin ', ' Fitness & Diet ']\n",
      "[' 54 ', ' Shadow of the Wind,The ', ' Zafon, Carlos Ruiz ', ' 1,092,349 ', ' Orion ', ' General & Literary Fiction ']\n",
      "[' 55 ', ' Tales of Beedle the Bard,The ', ' Rowling, J.K. ', ' 1,090,847 ', ' Bloomsbury ', \" Children's Fiction \"]\n",
      "[' 56 ', ' Broker,The ', ' Grisham, John ', ' 1,087,262 ', ' Random House ', ' Crime, Thriller & Adventure ']\n",
      "[' 57 ', \" Dr. Atkins' New Diet Revolution:The No-hunger, Luxurious Weight Loss P \", ' Atkins, Robert C. ', ' 1,054,196 ', ' Random House ', ' Fitness & Diet ']\n",
      "[' 58 ', ' Subtle Knife,The:His Dark Materials S. ', ' Pullman, Philip ', ' 1,037,160 ', ' Scholastic Ltd. ', ' Young Adult Fiction ']\n",
      "[' 59 ', ' Eats, Shoots and Leaves:The Zero Tolerance Approach to Punctuation ', ' Truss, Lynne ', ' 1,023,688 ', ' Profile Books Group ', ' Usage & Writing Guides ']\n",
      "[' 60 ', \" Delia's How to Cook:(Bk.1) \", ' Smith, Delia ', ' 1,015,956 ', ' Random House ', ' Food & Drink: General ']\n",
      "[' 61 ', ' Chocolat ', ' Harris, Joanne ', ' 1,009,873 ', ' Transworld ', ' General & Literary Fiction ']\n",
      "[' 62 ', ' Boy in the Striped Pyjamas,The ', ' Boyne, John ', ' 1,004,414 ', ' Random House Childrens Books G ', ' Young Adult Fiction ']\n",
      "[' 63 ', \" My Sister's Keeper \", ' Picoult, Jodi ', ' 1,003,780 ', ' Hodder & Stoughton ', ' General & Literary Fiction ']\n",
      "[' 64 ', ' Amber Spyglass,The:His Dark Materials S. ', ' Pullman, Philip ', ' 1,002,314 ', ' Scholastic Ltd. ', ' Young Adult Fiction ']\n",
      "[' 65 ', ' To Kill a Mockingbird ', ' Lee, Harper ', ' 998,213 ', ' Random House ', ' General & Literary Fiction ']\n",
      "[' 66 ', ' Men are from Mars, Women are from Venus:A Practical Guide for Improvin ', ' Gray, John ', ' 992,846 ', ' HarperCollins ', ' Popular Culture & Media: General Interest ']\n",
      "[' 67 ', ' Dear Fatty ', ' French, Dawn ', ' 986,753 ', ' Random House ', ' Autobiography: The Arts ']\n",
      "[' 68 ', ' Short History of Tractors in Ukrainian,A ', ' Lewycka, Marina ', ' 986,115 ', ' Penguin ', ' General & Literary Fiction ']\n",
      "[' 69 ', ' Hannibal ', ' Harris, Thomas ', ' 970,509 ', ' Random House ', ' Crime, Thriller & Adventure ']\n",
      "[' 70 ', ' Lord of the Rings,The ', ' Tolkien, J. R. R. ', ' 967,466 ', ' HarperCollins ', ' Science Fiction & Fantasy ']\n",
      "[' 71 ', ' Stupid White Men:...and Other Sorry Excuses for the State of the Natio ', ' Moore, Michael ', ' 963,353 ', ' Penguin ', ' Current Affairs & Issues ']\n",
      "[' 72 ', ' Interpretation of Murder,The ', ' Rubenfeld, Jed ', ' 962,515 ', ' Headline ', ' Crime, Thriller & Adventure ']\n",
      "[' 73 ', ' Sharon Osbourne Extreme:My Autobiography ', ' Osbourne, Sharon ', ' 959,496 ', ' Little, Brown Book ', ' Autobiography: The Arts ']\n",
      "[' 74 ', ' Alchemist,The:A Fable About Following Your Dream ', ' Coelho, Paulo ', ' 956,114 ', ' HarperCollins ', ' General & Literary Fiction ']\n",
      "[' 75 ', \" At My Mother's Knee ...:and Other Low Joints \", \" O'Grady, Paul \", ' 945,640 ', ' Transworld ', ' Autobiography: The Arts ']\n",
      "[' 76 ', ' Notes from a Small Island ', ' Bryson, Bill ', ' 931,312 ', ' Transworld ', ' Travel Writing ']\n",
      "[' 77 ', ' Return of the Naked Chef,The ', ' Oliver, Jamie ', ' 925,425 ', ' Penguin ', ' Food & Drink: General ']\n",
      "[' 78 ', ' Bridget Jones: The Edge of Reason ', ' Fielding, Helen ', ' 924,695 ', ' Pan Macmillan ', ' General & Literary Fiction ']\n",
      "[' 79 ', \" Jamie's Italy \", ' Oliver, Jamie ', ' 906,968 ', ' Penguin ', ' National & Regional Cuisine ']\n",
      "[' 80 ', ' I Can Make You Thin ', ' McKenna, Paul ', ' 905,086 ', ' Transworld ', ' Fitness & Diet ']\n",
      "[' 81 ', ' Down Under ', ' Bryson, Bill ', ' 890,847 ', ' Transworld ', ' Travel Writing ']\n",
      "[' 82 ', ' Summons,The ', ' Grisham, John ', ' 869,671 ', ' Random House ', ' Crime, Thriller & Adventure ']\n",
      "[' 83 ', ' Small Island ', ' Levy, Andrea ', ' 869,659 ', ' Headline ', ' General & Literary Fiction ']\n",
      "[' 84 ', ' Nigella Express ', ' Lawson, Nigella ', ' 862,602 ', ' Random House ', ' Food & Drink: General ']\n",
      "[' 85 ', ' Brick Lane ', ' Ali, Monica ', ' 856,540 ', ' Transworld ', ' General & Literary Fiction ']\n",
      "[' 86 ', \" Memory Keeper's Daughter,The \", ' Edwards, Kim ', ' 845,858 ', ' Penguin ', ' General & Literary Fiction ']\n",
      "[' 87 ', ' Room on the Broom ', ' Donaldson, Julia ', ' 842,535 ', ' Pan Macmillan ', ' Picture Books ']\n",
      "[' 88 ', ' About a Boy ', ' Hornby, Nick ', ' 828,215 ', ' Penguin ', ' General & Literary Fiction ']\n",
      "[' 89 ', ' My Booky Wook ', ' Brand, Russell ', ' 820,563 ', ' Hodder & Stoughton ', ' Autobiography: The Arts ']\n",
      "[' 90 ', ' God Delusion,The ', ' Dawkins, Richard ', ' 816,907 ', ' Transworld ', ' Popular Science ']\n",
      "[' 91 ', ' \"Beano\" Annual,The ', ' 0 ', ' 816,585 ', ' D.C. Thomson ', \" Children's Annuals \"]\n",
      "[' 92 ', ' White Teeth ', ' Smith, Zadie ', ' 815,586 ', ' Penguin ', ' General & Literary Fiction ']\n",
      "[' 93 ', ' House at Riverton,The ', ' Morton, Kate ', ' 814,370 ', ' Pan Macmillan ', ' General & Literary Fiction ']\n",
      "[' 94 ', ' Book Thief,The ', ' Zusak, Markus ', ' 809,641 ', ' Transworld ', ' General & Literary Fiction ']\n",
      "[' 95 ', ' Nights of Rain and Stars ', ' Binchy, Maeve ', ' 808,900 ', ' Orion ', ' General & Literary Fiction ']\n",
      "[' 96 ', ' Ghost,The ', ' Harris, Robert ', ' 807,311 ', ' Random House ', ' General & Literary Fiction ']\n",
      "[' 97 ', ' Happy Days with the Naked Chef ', ' Oliver, Jamie ', ' 794,201 ', ' Penguin ', ' Food & Drink: General ']\n",
      "[' 98 ', ' Hunger Games,The:Hunger Games Trilogy ', ' Collins, Suzanne ', ' 792,187 ', ' Scholastic Ltd. ', ' Young Adult Fiction ']\n",
      "[' 99 ', \" Lost Boy,The:A Foster Child's Search for the Love of a Family \", ' Pelzer, Dave ', ' 791,507 ', ' Orion ', ' Biography: General ']\n",
      "[' 100 ', \" Jamie's Ministry of Food:Anyone Can Learn to Cook in 24 Hours \", ' Oliver, Jamie ', ' 791,095 ', ' Penguin ', ' Food & Drink: General ']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.theguardian.com/news/datablog/2012/aug/09/best-selling-books-all-time-fifty-shades-grey-compare\"\n",
    "\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "table=soup.find('table', {'class':'in-article sortable'}).tbody\n",
    "\n",
    "rows=table.find_all('tr')\n",
    "\n",
    "columns=[v.text.replace('\\n','') for v in rows[0].find_all('th') ]\n",
    "\n",
    "df=pd.DataFrame(columns=columns)\n",
    "\n",
    "for i in range(0,len(rows)):\n",
    "    tds=rows[i].find_all('td')\n",
    "    values=[td.text.replace('\\n','') for td in tds]\n",
    "    print(values)  \n",
    "##df=df.append(pd.Series(values, index=columns),ignore_index=True)\n",
    "##    print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 9. Scrape the details most watched tv series of all time from imdb.com.\n",
    "Url = https://www.imdb.com/list/ls095964455/\n",
    "You have to find the following details:\n",
    "A) Name\n",
    "B) Year span\n",
    "C) Genre\n",
    "D) Run time\n",
    "E) Ratings\n",
    "F) Votes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Votes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Game of Thrones</td>\n",
       "      <td>(2011–2019)</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>57 min</td>\n",
       "      <td>9.3</td>\n",
       "      <td>1778772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stranger Things</td>\n",
       "      <td>(2016– )</td>\n",
       "      <td>Drama, Fantasy, Horror</td>\n",
       "      <td>51 min</td>\n",
       "      <td>8.7</td>\n",
       "      <td>829298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The Walking Dead</td>\n",
       "      <td>(2010– )</td>\n",
       "      <td>Drama, Horror, Thriller</td>\n",
       "      <td>44 min</td>\n",
       "      <td>8.2</td>\n",
       "      <td>857019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13 Reasons Why</td>\n",
       "      <td>(2017–2020)</td>\n",
       "      <td>Drama, Mystery, Thriller</td>\n",
       "      <td>60 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>257268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The 100</td>\n",
       "      <td>(2014–2020)</td>\n",
       "      <td>Drama, Mystery, Sci-Fi</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>217824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Orange Is the New Black</td>\n",
       "      <td>(2013–2019)</td>\n",
       "      <td>Comedy, Crime, Drama</td>\n",
       "      <td>59 min</td>\n",
       "      <td>8.1</td>\n",
       "      <td>279510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Riverdale</td>\n",
       "      <td>(2017– )</td>\n",
       "      <td>Crime, Drama, Mystery</td>\n",
       "      <td>45 min</td>\n",
       "      <td>6.9</td>\n",
       "      <td>119810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Grey's Anatomy</td>\n",
       "      <td>(2005– )</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>41 min</td>\n",
       "      <td>7.6</td>\n",
       "      <td>252577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The Flash</td>\n",
       "      <td>(2014– )</td>\n",
       "      <td>Action, Adventure, Drama</td>\n",
       "      <td>43 min</td>\n",
       "      <td>7.7</td>\n",
       "      <td>306409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Arrow</td>\n",
       "      <td>(2012–2020)</td>\n",
       "      <td>Action, Adventure, Crime</td>\n",
       "      <td>42 min</td>\n",
       "      <td>7.5</td>\n",
       "      <td>408386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Name         Year                     Genre Runtime  \\\n",
       "0          Game of Thrones  (2011–2019)  Action, Adventure, Drama  57 min   \n",
       "1          Stranger Things     (2016– )    Drama, Fantasy, Horror  51 min   \n",
       "2         The Walking Dead     (2010– )   Drama, Horror, Thriller  44 min   \n",
       "3           13 Reasons Why  (2017–2020)  Drama, Mystery, Thriller  60 min   \n",
       "4                  The 100  (2014–2020)    Drama, Mystery, Sci-Fi  43 min   \n",
       "5  Orange Is the New Black  (2013–2019)      Comedy, Crime, Drama  59 min   \n",
       "6                Riverdale     (2017– )     Crime, Drama, Mystery  45 min   \n",
       "7           Grey's Anatomy     (2005– )            Drama, Romance  41 min   \n",
       "8                The Flash     (2014– )  Action, Adventure, Drama  43 min   \n",
       "9                    Arrow  (2012–2020)  Action, Adventure, Crime  42 min   \n",
       "\n",
       "  Rating    Votes  \n",
       "0    9.3  1778772  \n",
       "1    8.7   829298  \n",
       "2    8.2   857019  \n",
       "3    7.6   257268  \n",
       "4    7.6   217824  \n",
       "5    8.1   279510  \n",
       "6    6.9   119810  \n",
       "7    7.6   252577  \n",
       "8    7.7   306409  \n",
       "9    7.5   408386  "
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "url=\"https://www.imdb.com/list/ls095964455/\"\n",
    "response=requests.get(url)\n",
    "\n",
    "soup=BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "movies=soup.findAll('div', {'class':'lister-item mode-detail'})\n",
    "\n",
    "Name=[]\n",
    "Year=[]\n",
    "Genre=[]\n",
    "Runtime=[]\n",
    "Rating=[]\n",
    "Votes=[]\n",
    "for i in movies:\n",
    "    Name.append(i.h3.a.text)\n",
    "    Year.append(i.find('span',{'class':'lister-item-year text-muted unbold'}).text)\n",
    "    Genre.append(i.find('span',{'class':'genre'}).text.strip())\n",
    "    Runtime.append(i.find(\"span\",{\"class\":\"runtime\"}).text)\n",
    "    Rating.append(i.find('span',{'class':'ipl-rating-star__rating'}).text)\n",
    "    Votes.append(i.find('span',{'name':'nv'})['data-value'])\n",
    "\n",
    "data=list(zip(Name,Year,Genre,Runtime,Rating,Votes))\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(data,columns=['Name','Year','Genre','Runtime','Rating','Votes'])\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 10. Details of Datasets from UCI machine learning repositories.\n",
    " Url = https://archive.ics.uci.edu/\n",
    " You have to find the following details:\n",
    "A) Dataset name\n",
    "B) Data type\n",
    "C) Task\n",
    "D) Attribute type\n",
    "E) No of instances\n",
    "F) No of attribute\n",
    "G) Year\n",
    "Note: - from the home page you have to go to the ShowAllDataset page through code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "PATH = \"C:\\Program Files (x86)\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(PATH)\n",
    "driver.get(\"https://archive.ics.uci.edu/ml/index.php\")\n",
    "\n",
    "try:\n",
    "    element=WebDriverWait(driver,10).until(\n",
    "    EC.presence_of_element_located((By.LINK_TEXT,\"View ALL Data Sets\"))\n",
    "    )\n",
    "    element.click()\n",
    "except:\n",
    "    driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Code for extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
