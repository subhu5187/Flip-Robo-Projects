{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text = requests.get(\"https://in.indeed.com/jobs?q=data%20analyst&l=Bangalore%2C%20Karnataka\").text\n",
    "soup = BeautifulSoup(html_text,'html.parser')\n",
    "jobs = soup.find_all('div',class_='jobsearch-SerpJobCard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = []\n",
    "company = []\n",
    "summary = []\n",
    "job_location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for job in jobs:\n",
    "    atag = job.h2.a\n",
    "    job_title = atag.get('title')\n",
    "    job_name.append(job_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in jobs:\n",
    "    ctag = x.find('span','company').text.strip()\n",
    "    company.append(ctag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for location in jobs:\n",
    "    place = location.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    job_location.append(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs:\n",
    "    sm = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    summary.append(sm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist / Machine Learning Jobs in Noid...</td>\n",
       "      <td>Trane Technologies</td>\n",
       "      <td>Interact with data analyst I, database develop...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist Engineer Jobs in India, Noida</td>\n",
       "      <td>Indeed</td>\n",
       "      <td>Advanced knowledge of a known data visualizati...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst/ Scientist Jobs in India, Noida</td>\n",
       "      <td>PayPal</td>\n",
       "      <td>Create and maintain data pipelines on data ana...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analytics (Advanced Excel) Internship</td>\n",
       "      <td>Trane Technologies</td>\n",
       "      <td>Interact with data analyst I, database develop...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deep Learning / Computer Vision Engineer</td>\n",
       "      <td>KellyOCG</td>\n",
       "      <td>Ability to analyse and process the data. Good ...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Volvo Group</td>\n",
       "      <td>Build expertise in data sources to find, manag...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Proziod Analytics Pvt Ltd</td>\n",
       "      <td>\\* Proven working experience as a data analyst...</td>\n",
       "      <td>Koramangala, Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data science</td>\n",
       "      <td>NetApp</td>\n",
       "      <td>Evaluate the ROI for the data enriched. Captur...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science Trainer</td>\n",
       "      <td>Myntra.com</td>\n",
       "      <td>A data analyst will need to own projects from ...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Machine Learning and Deep learning expert</td>\n",
       "      <td>Jiva Sciences Private Limited</td>\n",
       "      <td>Qualification - B.sc ( Prefer Physics , statis...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           JOB_TITLE  \\\n",
       "0  Data Scientist / Machine Learning Jobs in Noid...   \n",
       "1       Data Scientist Engineer Jobs in India, Noida   \n",
       "2       Data Analyst/ Scientist Jobs in India, Noida   \n",
       "3         Data Analytics (Advanced Excel) Internship   \n",
       "4           Deep Learning / Computer Vision Engineer   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                                       Data science   \n",
       "8                               Data Science Trainer   \n",
       "9          Machine Learning and Deep learning expert   \n",
       "\n",
       "                         COMPANY  \\\n",
       "0             Trane Technologies   \n",
       "1                         Indeed   \n",
       "2                         PayPal   \n",
       "3             Trane Technologies   \n",
       "4                       KellyOCG   \n",
       "5                    Volvo Group   \n",
       "6      Proziod Analytics Pvt Ltd   \n",
       "7                         NetApp   \n",
       "8                     Myntra.com   \n",
       "9  Jiva Sciences Private Limited   \n",
       "\n",
       "                                        EXPECTATIONS  \\\n",
       "0  Interact with data analyst I, database develop...   \n",
       "1  Advanced knowledge of a known data visualizati...   \n",
       "2  Create and maintain data pipelines on data ana...   \n",
       "3  Interact with data analyst I, database develop...   \n",
       "4  Ability to analyse and process the data. Good ...   \n",
       "5  Build expertise in data sources to find, manag...   \n",
       "6  \\* Proven working experience as a data analyst...   \n",
       "7  Evaluate the ROI for the data enriched. Captur...   \n",
       "8  A data analyst will need to own projects from ...   \n",
       "9  Qualification - B.sc ( Prefer Physics , statis...   \n",
       "\n",
       "                            LOCATION  \n",
       "0               Bengaluru, Karnataka  \n",
       "1               Bengaluru, Karnataka  \n",
       "2               Bengaluru, Karnataka  \n",
       "3               Bengaluru, Karnataka  \n",
       "4               Bengaluru, Karnataka  \n",
       "5               Bengaluru, Karnataka  \n",
       "6  Koramangala, Bengaluru, Karnataka  \n",
       "7               Bengaluru, Karnataka  \n",
       "8               Bengaluru, Karnataka  \n",
       "9               Bengaluru, Karnataka  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "weather = pd.DataFrame({\n",
    "        \"JOB_TITLE\": job_name[:10],\n",
    "         \"COMPANY\": company[:10],\n",
    "         \"EXPECTATIONS\": summary[:10],\n",
    "         \"LOCATION\": job_location[:10]\n",
    "         \n",
    "    })\n",
    "weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.\n",
    "Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text1 = requests.get(\"https://in.indeed.com/jobs?q=data%20scientist&l=Bangalore%2C%20Karnataka&advn=7641058883739924&vjk=6dc1a9b79905765e\").text\n",
    "soup1 = BeautifulSoup(html_text1,'html.parser')\n",
    "jobs1 = soup1.find_all('div',class_='jobsearch-SerpJobCard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = []\n",
    "Company = []\n",
    "Summary = []\n",
    "location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs1:\n",
    "    btag = i.h2.a\n",
    "    title = btag.get('title')\n",
    "    name.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in jobs1:\n",
    "    Ctag = x.find('span','company').text.strip()\n",
    "    Company.append(Ctag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pla in jobs1:\n",
    "    place = pla.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    location.append(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs1:\n",
    "    sam = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    Summary.append(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>KellyOCG</td>\n",
       "      <td>Investigate data quality and clean data where ...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>HP</td>\n",
       "      <td>We are looking for self-driven &amp; expert data s...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Statistical Forecasting - Data Scientist</td>\n",
       "      <td>HP</td>\n",
       "      <td>HP is the world’s leading personal systems and...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Research Intern (Computer Vision &amp; Machine Lea...</td>\n",
       "      <td>Avian Aerospace Pvt Ltd</td>\n",
       "      <td>We are a fast growing Technology Startup based...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Scientist - Big Data OPS</td>\n",
       "      <td>HP</td>\n",
       "      <td>Ensures data integrity and data quality of the...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>PayPal</td>\n",
       "      <td> Strong in data science, analytics and proble...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Experienced Data Scientist</td>\n",
       "      <td>BOEING</td>\n",
       "      <td>Ensure data quality and integrity, Interpret a...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist IV</td>\n",
       "      <td>Conduent</td>\n",
       "      <td>Helps other data scientists understand the bro...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sr. Data Scientist</td>\n",
       "      <td>Availity</td>\n",
       "      <td>Having at least 3 years of hands on experience...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Scientist I/II</td>\n",
       "      <td>Philips</td>\n",
       "      <td>As data scientist you will investigate, discer...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Data Scientist -Machine Learning (Commerce BU)</td>\n",
       "      <td>Blue Yonder</td>\n",
       "      <td>Independently, or alongside junior scientists,...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Schneider Electric</td>\n",
       "      <td>&lt;&gt;. &lt;&gt;.</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Data Scientist - Fuel Economy</td>\n",
       "      <td>Mercedes-Benz Research and Development India P...</td>\n",
       "      <td>Experience in data modelling, data analysis, d...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Snr Data Scientist-NBFC(Financial services)-Ba...</td>\n",
       "      <td>Angel &amp; Genie</td>\n",
       "      <td>Excellent communication skills (written, verba...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Commonwealth Bank</td>\n",
       "      <td>Deliver business value by interpreting data (i...</td>\n",
       "      <td>Bengaluru, Karnataka</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            JOB_TITLE  \\\n",
       "0                                      Data Scientist   \n",
       "1                                      Data Scientist   \n",
       "2            Statistical Forecasting - Data Scientist   \n",
       "3   Research Intern (Computer Vision & Machine Lea...   \n",
       "4                       Data Scientist - Big Data OPS   \n",
       "5                                      Data Scientist   \n",
       "6                          Experienced Data Scientist   \n",
       "7                                   Data Scientist IV   \n",
       "8                                  Sr. Data Scientist   \n",
       "9                                 Data Scientist I/II   \n",
       "10     Data Scientist -Machine Learning (Commerce BU)   \n",
       "11                                Lead Data Scientist   \n",
       "12                      Data Scientist - Fuel Economy   \n",
       "13  Snr Data Scientist-NBFC(Financial services)-Ba...   \n",
       "14                                     Data Scientist   \n",
       "\n",
       "                                              COMPANY  \\\n",
       "0                                            KellyOCG   \n",
       "1                                                  HP   \n",
       "2                                                  HP   \n",
       "3                             Avian Aerospace Pvt Ltd   \n",
       "4                                                  HP   \n",
       "5                                              PayPal   \n",
       "6                                              BOEING   \n",
       "7                                            Conduent   \n",
       "8                                            Availity   \n",
       "9                                             Philips   \n",
       "10                                        Blue Yonder   \n",
       "11                                 Schneider Electric   \n",
       "12  Mercedes-Benz Research and Development India P...   \n",
       "13                                      Angel & Genie   \n",
       "14                                  Commonwealth Bank   \n",
       "\n",
       "                                         EXPECTATIONS              LOCATION  \n",
       "0   Investigate data quality and clean data where ...  Bengaluru, Karnataka  \n",
       "1   We are looking for self-driven & expert data s...  Bengaluru, Karnataka  \n",
       "2   HP is the world’s leading personal systems and...  Bengaluru, Karnataka  \n",
       "3   We are a fast growing Technology Startup based...  Bengaluru, Karnataka  \n",
       "4   Ensures data integrity and data quality of the...  Bengaluru, Karnataka  \n",
       "5    Strong in data science, analytics and proble...  Bengaluru, Karnataka  \n",
       "6   Ensure data quality and integrity, Interpret a...  Bengaluru, Karnataka  \n",
       "7   Helps other data scientists understand the bro...  Bengaluru, Karnataka  \n",
       "8   Having at least 3 years of hands on experience...  Bengaluru, Karnataka  \n",
       "9   As data scientist you will investigate, discer...  Bengaluru, Karnataka  \n",
       "10  Independently, or alongside junior scientists,...  Bengaluru, Karnataka  \n",
       "11                                            <>. <>.  Bengaluru, Karnataka  \n",
       "12  Experience in data modelling, data analysis, d...  Bengaluru, Karnataka  \n",
       "13  Excellent communication skills (written, verba...  Bengaluru, Karnataka  \n",
       "14  Deliver business value by interpreting data (i...  Bengaluru, Karnataka  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "qwe = pd.DataFrame({\n",
    "        \"JOB_TITLE\": name[:10],\n",
    "         \"COMPANY\": Company[:10],\n",
    "         \"EXPECTATIONS\": Summary[:10],\n",
    "         \"LOCATION\": location[:10]\n",
    "    })\n",
    "qwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.\n",
    "Scrape data for “Data Scientist” designation for first 10 job results from Naukri.com "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_text2 = requests.get(\"https://in.indeed.com/jobs?q=data%20scientist%20%E2%82%B93%2C00%2C000&l=Delhi&ts=1613473497941&rq=1&rsIdx=1\").text\n",
    "soup2 = BeautifulSoup(html_text2,'html.parser')\n",
    "jobs2 = soup2.find_all('div',class_='jobsearch-SerpJobCard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = []\n",
    "Company_name = []\n",
    "Summary = []\n",
    "Location = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs2:\n",
    "    ytag = i.h2.a\n",
    "    title = ytag.get('title')\n",
    "    job_name.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in jobs2:\n",
    "    otag = x.find('span','company').text.strip()\n",
    "    Company_name.append(otag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pl in jobs2:\n",
    "    Place = pl.find('div', 'recJobLoc').get('data-rc-loc')\n",
    "    Location.append(Place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in jobs2:\n",
    "    sym = i.find('div', 'summary').text.strip().replace('\\n', ' ')\n",
    "    Summary.append(sym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>JOB_TITLE</th>\n",
       "      <th>COMPANY</th>\n",
       "      <th>EXPECTATIONS</th>\n",
       "      <th>LOCATION</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Scientist / Machine Learning Jobs in Noid...</td>\n",
       "      <td>ANI Calls India Private Limited</td>\n",
       "      <td>Strong intuition for data and Keen aptitude on...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Scientist Engineer Jobs in India, Noida</td>\n",
       "      <td>ANI Calls India Private Limited</td>\n",
       "      <td>Job Description : 5 years of professional expe...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Analyst/ Scientist Jobs in India, Noida</td>\n",
       "      <td>ANI Calls India Private Limited</td>\n",
       "      <td>Design, develop and implement data science and...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Analytics (Advanced Excel) Internship</td>\n",
       "      <td>Credgenics</td>\n",
       "      <td>Selected intern's day-to-day responsibilities ...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Deep Learning / Computer Vision Engineer</td>\n",
       "      <td>Agrex Technologies Private Limited</td>\n",
       "      <td>Understanding of data structures, data modelin...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Terra Economics &amp; Analytics Lab (TEAL)</td>\n",
       "      <td>Creating compelling data visualizations and co...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>ESRI, Inc.</td>\n",
       "      <td>Our team develops tools, APIs, and AI models f...</td>\n",
       "      <td>New Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data science</td>\n",
       "      <td>Badatya Private Limited</td>\n",
       "      <td>Data Scientist For Trainer Role*. We are looki...</td>\n",
       "      <td>South Extn Part I, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Science Trainer</td>\n",
       "      <td>onfo digitalways Pvt. Ltd.</td>\n",
       "      <td>Required Data Science Trainer cum Teacher for ...</td>\n",
       "      <td>Uttam Nagar, Delhi, Delhi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Machine Learning and Deep learning expert</td>\n",
       "      <td>Baoiam</td>\n",
       "      <td>Need Machine learning/Deep learning expert. Pa...</td>\n",
       "      <td>Delhi, Delhi</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           JOB_TITLE  \\\n",
       "0  Data Scientist / Machine Learning Jobs in Noid...   \n",
       "1       Data Scientist Engineer Jobs in India, Noida   \n",
       "2       Data Analyst/ Scientist Jobs in India, Noida   \n",
       "3         Data Analytics (Advanced Excel) Internship   \n",
       "4           Deep Learning / Computer Vision Engineer   \n",
       "5                                     Data Scientist   \n",
       "6                                     Data Scientist   \n",
       "7                                       Data science   \n",
       "8                               Data Science Trainer   \n",
       "9          Machine Learning and Deep learning expert   \n",
       "\n",
       "                                  COMPANY  \\\n",
       "0         ANI Calls India Private Limited   \n",
       "1         ANI Calls India Private Limited   \n",
       "2         ANI Calls India Private Limited   \n",
       "3                              Credgenics   \n",
       "4      Agrex Technologies Private Limited   \n",
       "5  Terra Economics & Analytics Lab (TEAL)   \n",
       "6                              ESRI, Inc.   \n",
       "7                 Badatya Private Limited   \n",
       "8              onfo digitalways Pvt. Ltd.   \n",
       "9                                  Baoiam   \n",
       "\n",
       "                                        EXPECTATIONS  \\\n",
       "0  Strong intuition for data and Keen aptitude on...   \n",
       "1  Job Description : 5 years of professional expe...   \n",
       "2  Design, develop and implement data science and...   \n",
       "3  Selected intern's day-to-day responsibilities ...   \n",
       "4  Understanding of data structures, data modelin...   \n",
       "5  Creating compelling data visualizations and co...   \n",
       "6  Our team develops tools, APIs, and AI models f...   \n",
       "7  Data Scientist For Trainer Role*. We are looki...   \n",
       "8  Required Data Science Trainer cum Teacher for ...   \n",
       "9  Need Machine learning/Deep learning expert. Pa...   \n",
       "\n",
       "                    LOCATION  \n",
       "0               Delhi, Delhi  \n",
       "1               Delhi, Delhi  \n",
       "2               Delhi, Delhi  \n",
       "3               Delhi, Delhi  \n",
       "4               Delhi, Delhi  \n",
       "5           New Delhi, Delhi  \n",
       "6           New Delhi, Delhi  \n",
       "7   South Extn Part I, Delhi  \n",
       "8  Uttam Nagar, Delhi, Delhi  \n",
       "9               Delhi, Delhi  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "twe = pd.DataFrame({\n",
    "        \"JOB_TITLE\": job_name[:10],\n",
    "         \"COMPANY\": Company_name[:10],\n",
    "         \"EXPECTATIONS\": Summary[:10],\n",
    "         \"LOCATION\": Location[:10]\n",
    "    })\n",
    "twe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\n",
    "Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand1 = []\n",
    "Product_Description1 = []\n",
    "Price1 = []\n",
    "Discount1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pags = list(range(1,5))\n",
    "for page in pags:\n",
    "    requr = requests.get(\"https://www.flipkart.com/search?q=sunglasses&sid=26x&as=on&as-show=on&otracker=AS_QueryStore_HistoryAutoSuggest_1_4_na_na_na&otracker1=AS_QueryStore_HistoryAutoSuggest_1_4_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=sunglasses%7CSunglasses&requestId=ed6f54e5-30b5-43c9-80e9-476306844180&as-searchtext=sung\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup9 = BeautifulSoup(requr,'html.parser')\n",
    "    des = soup9.find_all('div' , class_='_2WkVRV')\n",
    "    for i in range(len(des)):\n",
    "        Brand1.append(des[i].text)\n",
    "    len(Brand1)\n",
    "    price = soup9.find_all('div',class_='_30jeq3') \n",
    "    for i in range(len(price)):\n",
    "        Price1.append(price[i].text)\n",
    "        len(Price1)\n",
    "    xc = soup9.find_all('a',class_='IRpwTa') \n",
    "    for i in range(len(xc)):\n",
    "        Product_Description1.append(xc[i].text)\n",
    "        len(xc)\n",
    "    cv = soup9.find_all('div',class_='_3Ay6Sb')\n",
    "    for i in range(len(cv)):\n",
    "        Discount1.append(cv[i].text)\n",
    "        len(cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESC</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hipe</td>\n",
       "      <td>UV Protection, Mirrored Round Sunglasses (Free...</td>\n",
       "      <td>₹169</td>\n",
       "      <td>87% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HIPPON</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (55)</td>\n",
       "      <td>₹198</td>\n",
       "      <td>83% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Gradient, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>37% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elligator</td>\n",
       "      <td>UV Protection Round Sunglasses (54)</td>\n",
       "      <td>₹295</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Phenomenal</td>\n",
       "      <td>UV Protection, Mirrored Retro Square Sunglasse...</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>ETRG</td>\n",
       "      <td>Gradient, UV Protection Aviator Sunglasses (99)</td>\n",
       "      <td>₹295</td>\n",
       "      <td>88% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>shah collections</td>\n",
       "      <td>UV Protection, Polarized, Mirrored Rectangular...</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROZZETTA CRAFT</td>\n",
       "      <td>UV Protection Retro Square Sunglasses (Free Size)</td>\n",
       "      <td>₹399</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Online Mantra</td>\n",
       "      <td>UV Protection Round, Aviator Sunglasses (Free ...</td>\n",
       "      <td>₹599</td>\n",
       "      <td>33% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹331</td>\n",
       "      <td>80% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               BRAND                                       PRODUCT_DESC PRICE  \\\n",
       "0               hipe  UV Protection, Mirrored Round Sunglasses (Free...  ₹169   \n",
       "1             HIPPON             UV Protection Wayfarer Sunglasses (55)  ₹198   \n",
       "2           Fastrack  Gradient, UV Protection Wayfarer Sunglasses (F...  ₹499   \n",
       "3          Elligator                UV Protection Round Sunglasses (54)  ₹295   \n",
       "4         Phenomenal  UV Protection, Mirrored Retro Square Sunglasse...  ₹399   \n",
       "..               ...                                                ...   ...   \n",
       "95              ETRG    Gradient, UV Protection Aviator Sunglasses (99)  ₹295   \n",
       "96  shah collections  UV Protection, Polarized, Mirrored Rectangular...  ₹399   \n",
       "97    ROZZETTA CRAFT  UV Protection Retro Square Sunglasses (Free Size)  ₹399   \n",
       "98     Online Mantra  UV Protection Round, Aviator Sunglasses (Free ...  ₹599   \n",
       "99          Fastrack  Mirrored, UV Protection Wayfarer Sunglasses (F...  ₹331   \n",
       "\n",
       "   DISCOUNT  \n",
       "0   87% off  \n",
       "1   83% off  \n",
       "2   37% off  \n",
       "3   88% off  \n",
       "4   80% off  \n",
       "..      ...  \n",
       "95  88% off  \n",
       "96  80% off  \n",
       "97  80% off  \n",
       "98  33% off  \n",
       "99  80% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "kwe = pd.DataFrame({\n",
    "        \"BRAND\": Brand1[:100],\n",
    "         \"PRODUCT_DESC\": Product_Description1[:100],\n",
    "         \"PRICE\": Price1[:100],\n",
    "         \"DISCOUNT\": Discount1[:100]\n",
    "    })\n",
    "kwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\n",
    "Scrape 100 reviews data from flipkart.com for iphone11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Rating = []\n",
    "Review = []\n",
    "Review_Summary = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pag = list(range(1,12))\n",
    "for page in pag:\n",
    "    reque = requests.get(\"https://www.flipkart.com/apple-iphone-11-white-64-gb/product-reviews/itmfc6a7091eb20b?pid=MOBFWQ6BVWVEH3XE&lid=LSTMOBFWQ6BVWVEH3XESAHPTP&marketplace=FLIPKART\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup6 = BeautifulSoup(reque,'html.parser')\n",
    "    qwer = soup6.find_all('div' , class_='_3LWZlK _1BLPMq')\n",
    "    for i in range(len(qwer)):\n",
    "        Rating.append(qwer[i].text)\n",
    "    len(Rating)\n",
    "    rev = soup6.find_all('p',class_='_2-N8zT') \n",
    "    for i in range(len(rev)):\n",
    "        Review.append(rev[i].text)\n",
    "        len(Review)\n",
    "    rs = soup6.find_all('div',class_='t-ZTKy') \n",
    "    for i in range(len(rs)):\n",
    "        Review_Summary.append(rs[i].text)\n",
    "        len(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING</th>\n",
       "      <th>REVIEW</th>\n",
       "      <th>REVIEW_SUMMARY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.I’m am ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>It’s a must buy who is looking for an upgrade ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the MoneyThe iPhone 11 offe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Value for money❤️❤️Its awesome mobile phone in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Best budget Iphone till date ❤️ go for it guys...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>It’s been almost a month since I have been usi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>*Review after 10 months of usage*Doesn't seem ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>4</td>\n",
       "      <td>Nice product</td>\n",
       "      <td>Awesome Phone. Slightly high price but worth. ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RATING              REVIEW  \\\n",
       "0       5    Perfect product!   \n",
       "1       5       Great product   \n",
       "2       5  Highly recommended   \n",
       "3       5    Perfect product!   \n",
       "4       5           Brilliant   \n",
       "..    ...                 ...   \n",
       "85      5    Perfect product!   \n",
       "86      5   Worth every penny   \n",
       "87      5   Worth every penny   \n",
       "88      5           Wonderful   \n",
       "89      4        Nice product   \n",
       "\n",
       "                                       REVIEW_SUMMARY  \n",
       "0   Amazing phone with great cameras and better ba...  \n",
       "1   Amazing Powerful and Durable Gadget.I’m am ver...  \n",
       "2   iphone 11 is a very good phone to buy only if ...  \n",
       "3   It’s a must buy who is looking for an upgrade ...  \n",
       "4   The Best Phone for the MoneyThe iPhone 11 offe...  \n",
       "..                                                ...  \n",
       "85  Value for money❤️❤️Its awesome mobile phone in...  \n",
       "86  Best budget Iphone till date ❤️ go for it guys...  \n",
       "87  It’s been almost a month since I have been usi...  \n",
       "88  *Review after 10 months of usage*Doesn't seem ...  \n",
       "89  Awesome Phone. Slightly high price but worth. ...  \n",
       "\n",
       "[90 rows x 3 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pwe = pd.DataFrame({\n",
    "        \"RATING\": Rating[:100],\n",
    "         \"REVIEW\": Review[:100],\n",
    "         \"REVIEW_SUMMARY\": Review_Summary[:100],\n",
    "    })\n",
    "pwe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.\n",
    "Scrape data for first 100 sneakers you find when you visit flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Brand = []\n",
    "Product_Description = []\n",
    "Price = []\n",
    "discount = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pags = list(range(1,5))\n",
    "for page in pags:\n",
    "    reqe = requests.get(\"https://www.flipkart.com/search?q=sneakers&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup5 = BeautifulSoup(reqe,'html.parser')\n",
    "    descr = soup5.find_all('div' , class_='_2WkVRV')\n",
    "    for i in range(len(descr)):\n",
    "        Brand.append(descr[i].text)\n",
    "    len(Brand)\n",
    "    price = soup5.find_all('div',class_='_30jeq3') \n",
    "    for i in range(len(price)):\n",
    "        Price.append(price[i].text)\n",
    "        len(Price)\n",
    "    xc = soup5.find_all('a',class_='IRpwTa') \n",
    "    for i in range(len(xc)):\n",
    "        Product_Description.append(xc[i].text)\n",
    "        len(xc)\n",
    "    cv = soup5.find_all('div',class_='_3Ay6Sb')\n",
    "    for i in range(len(cv)):\n",
    "        discount.append(cv[i].text)\n",
    "        len(cv)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BRAND</th>\n",
       "      <th>PRODUCT_DESC</th>\n",
       "      <th>PRICE</th>\n",
       "      <th>DISCOUNT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kizaar</td>\n",
       "      <td>Fashionable Casual, Canvas,official or Partywe...</td>\n",
       "      <td>₹411</td>\n",
       "      <td>58% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crysta</td>\n",
       "      <td>1245SpiderBlack Sneakers For Men</td>\n",
       "      <td>₹270</td>\n",
       "      <td>45% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹449</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>World Wear Footwear</td>\n",
       "      <td>Combo Pack of 4 Latest Collection Stylish Casu...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Smart Casuals Canvas Shoes Combo pack of 2 Sne...</td>\n",
       "      <td>₹349</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Axter</td>\n",
       "      <td>Combo Pack of 2 Latest Collection Stylish Casu...</td>\n",
       "      <td>₹398</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Aura</td>\n",
       "      <td>Trendy and Stylish Combo Pack of 3 Casual Shoe...</td>\n",
       "      <td>₹236</td>\n",
       "      <td>52% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Combo Pack Of 4 Canvas Sneakers For Men</td>\n",
       "      <td>₹242</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Red Tape</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹630</td>\n",
       "      <td>64% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  BRAND                                       PRODUCT_DESC  \\\n",
       "0                Kizaar  Fashionable Casual, Canvas,official or Partywe...   \n",
       "1                Crysta                   1245SpiderBlack Sneakers For Men   \n",
       "2                Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...   \n",
       "3          Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men   \n",
       "4   World Wear Footwear  Combo Pack of 4 Latest Collection Stylish Casu...   \n",
       "..                  ...                                                ...   \n",
       "95               Chevit  Smart Casuals Canvas Shoes Combo pack of 2 Sne...   \n",
       "96                Axter  Combo Pack of 2 Latest Collection Stylish Casu...   \n",
       "97                 Aura  Trendy and Stylish Combo Pack of 3 Casual Shoe...   \n",
       "98               BRUTON            Combo Pack Of 4 Canvas Sneakers For Men   \n",
       "99             Red Tape                                   Sneakers For Men   \n",
       "\n",
       "   PRICE DISCOUNT  \n",
       "0   ₹411  58% off  \n",
       "1   ₹270  45% off  \n",
       "2   ₹449  77% off  \n",
       "3   ₹399  60% off  \n",
       "4   ₹474  76% off  \n",
       "..   ...      ...  \n",
       "95  ₹349  65% off  \n",
       "96  ₹398  60% off  \n",
       "97  ₹236  52% off  \n",
       "98  ₹242  65% off  \n",
       "99  ₹630  64% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "awe = pd.DataFrame({\n",
    "        \"BRAND\": Brand[:100],\n",
    "         \"PRODUCT_DESC\": Product_Description[:100],\n",
    "         \"PRICE\": Price[:100],\n",
    "         \"DISCOUNT\": discount[:100]\n",
    "    })\n",
    "awe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.\n",
    "Scrape data for first 10 Laptops you find when you visit flipkart.com."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = []\n",
    "prices = []\n",
    "ratings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = list(range(1,2))\n",
    "for page in pages:\n",
    "    req = requests.get(\"https://www.flipkart.com/search?q=laptops&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page={}\".format(page)).text  # URL of the website which you want to scrape\n",
    "    soup4 = BeautifulSoup(req,'html.parser')\n",
    "    desc = soup4.find_all('div' , class_='_4rR01T')\n",
    "    for i in range(len(desc)):\n",
    "        descriptions.append(desc[i].text)\n",
    "    len(descriptions)\n",
    "    price = soup4.find_all('div',class_='_30jeq3 _1_WHN1') \n",
    "    for i in range(len(price)):\n",
    "        prices.append(price[i].text)\n",
    "        len(prices)\n",
    "\n",
    "    rating = soup4.find_all('div',class_='_3LWZlK') \n",
    "    for i in range(len(rating)):\n",
    "        ratings.append(rating[i].text)\n",
    "        len(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>RATING</th>\n",
       "      <th>PRICE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Asus ROG Zephyrus G Ryzen 7 Quad Core 3750H - ...</td>\n",
       "      <td>4.2</td>\n",
       "      <td>₹84,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Asus ZenBook Flip 14 Ryzen 5 Quad Core 3500U 2...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>₹65,711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP 15s Celeron Dual Core - (4 GB/1 TB HDD/Wind...</td>\n",
       "      <td>4</td>\n",
       "      <td>₹23,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lenovo Ideapad Gaming 3 Ryzen 5 Hexa Core 4600...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>₹59,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Dell Inspiron 3505 Ryzen 3 Dual Core 3250U - (...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>₹34,790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MSI Modern 14 Ryzen 5 Hexa Core 4500U - (8 GB/...</td>\n",
       "      <td>4.7</td>\n",
       "      <td>₹51,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP 15 Ryzen 3 Dual Core 3200U - (4 GB/1 TB HDD...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>₹30,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HP 14 Core i5 10th Gen - (8 GB/512 GB SSD/Wind...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹47,490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Asus Core i3 10th Gen - (4 GB/512 GB SSD/Windo...</td>\n",
       "      <td>4.3</td>\n",
       "      <td>₹36,990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Asus VivoBook 14 Ryzen 5 Quad Core 3500U - (8 ...</td>\n",
       "      <td>4.4</td>\n",
       "      <td>₹43,990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE RATING    PRICE\n",
       "0  Asus ROG Zephyrus G Ryzen 7 Quad Core 3750H - ...    4.2  ₹84,990\n",
       "1  Asus ZenBook Flip 14 Ryzen 5 Quad Core 3500U 2...    4.4  ₹65,711\n",
       "2  HP 15s Celeron Dual Core - (4 GB/1 TB HDD/Wind...      4  ₹23,990\n",
       "3  Lenovo Ideapad Gaming 3 Ryzen 5 Hexa Core 4600...    4.5  ₹59,990\n",
       "4  Dell Inspiron 3505 Ryzen 3 Dual Core 3250U - (...    4.4  ₹34,790\n",
       "5  MSI Modern 14 Ryzen 5 Hexa Core 4500U - (8 GB/...    4.7  ₹51,990\n",
       "6  HP 15 Ryzen 3 Dual Core 3200U - (4 GB/1 TB HDD...    4.1  ₹30,990\n",
       "7  HP 14 Core i5 10th Gen - (8 GB/512 GB SSD/Wind...    4.3  ₹47,490\n",
       "8  Asus Core i3 10th Gen - (4 GB/512 GB SSD/Windo...    4.3  ₹36,990\n",
       "9  Asus VivoBook 14 Ryzen 5 Quad Core 3500U - (8 ...    4.4  ₹43,990"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dwe = pd.DataFrame({\n",
    "        \"TITLE\": descriptions[:10],\n",
    "         \"RATING\": ratings[:10],\n",
    "         \"PRICE\": prices[:10],\n",
    "    })\n",
    "dwe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
